{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Consumer and Sentiment Analysis\n",
    "\n",
    "**Original Author:** Walker Rowe.<br/>\n",
    "**With modification of:** Astrid Krickl.<br/>\n",
    "**Additional Info:** Working with Streaming Twitter Data Using Kafka. https://www.bmc.com/blogs/working-streaming-twitter-data-using-kafka/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the os module \n",
    "import os\n",
    "\n",
    "# Set the PYSPARK_SUBMIT_ARGS to the appropriate spark-sql-kafka package\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install findspark\n",
    "!{sys.executable} -m pip install -U textblob\n",
    "!{sys.executable} -m pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the findspark module \n",
    "import findspark\n",
    "\n",
    "# Initialize via the full spark path\n",
    "findspark.init(\"/usr/local/spark/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating help functions for processing of the tweets\n",
    "* Function prepocessing\n",
    "    * Extract from json created_at, text and user\n",
    "    * Extract from user screen_name and location\n",
    "    * Clean tweets \n",
    "* Function polarity_detection\n",
    "    * Returns polarity of tweets\n",
    "* Function sebjectivity_detection\n",
    "    * Returns subjectivity of tweets\n",
    "* Function text_classification\n",
    "    * Append polarity and subjectivity to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(lines):\n",
    "    # Just select the tweet text itself\n",
    "    words = lines.select(json_tuple('json_data', 'created_at','text', 'user').alias('created_at', 'word', 'json_user')) \n",
    "\n",
    "    # extract screen_name and location from user info\n",
    "    words = words.select('word', 'created_at', json_tuple('json_user', 'screen_name', 'location').alias('screen_name', 'location'))\n",
    "    \n",
    "    # Clean up the tweets\n",
    "    words = words.na.replace('', None)\n",
    "    words = words.na.drop()\n",
    "    words = words.withColumn('word', F.regexp_replace('word', r'http\\S+', ''))\n",
    "    words = words.withColumn('word', F.regexp_replace('word', '@\\w+', ''))\n",
    "    words = words.withColumn('word', F.regexp_replace('word', '#', ''))\n",
    "    words = words.withColumn('word', F.regexp_replace('word', 'RT', ''))\n",
    "    words = words.withColumn('word', F.regexp_replace('word', ':', ''))\n",
    "    return words\n",
    "\n",
    "# Define methods from TextBlob\n",
    "def polarity_detection(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def subjectivity_detection(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# We need to create user defined functions for the Textblob methods in order to use them\n",
    "def text_classification(words):\n",
    "    # polarity detection\n",
    "    # Define as user defined fuction to embed method in the spark environment \n",
    "    polarity_detection_udf = udf(polarity_detection, StringType())\n",
    "    # Append polarity to dataframe\n",
    "    words = words.withColumn(\"polarity\", polarity_detection_udf(\"word\"))\n",
    "    \n",
    "    # subjectivity detection\n",
    "    # Define as user defined fuction to embed method in the spark environment \n",
    "    subjectivity_detection_udf = udf(subjectivity_detection, StringType())\n",
    "    # Append subjectivity to dataframe \n",
    "    words = words.withColumn(\"subjectivity\", subjectivity_detection_udf(\"word\"))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spark sql and sql.types, and sql.functions, time and textblob modules \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, split\n",
    "import time\n",
    "from pyspark.sql import functions as F #NEW\n",
    "from textblob import TextBlob   #NEW\n",
    "\n",
    "# Gets an existing :class:`SparkSession` or, if there is no existing one, creates a new one based on the options set in this builder.\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local[*]\") \\\n",
    "   .appName(\"WeatherApp\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Read Stream from Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encapsulate the code in try except blocks\n",
    "try:\n",
    "    # Interface used to load a streaming :class:`DataFrame <pyspark.sql.DataFrame>` from external storage systems (e.g. file systems, key-value stores, etc).\n",
    "    tweet_df = spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "        .option(\"subscribe\", \"weather\") \\\n",
    "        .option(\"startingOffsets\", \"latest\")  \\\n",
    "        .load()   \n",
    "except:\n",
    "    # Print the error\n",
    "    print(\"Unexpected error:\", sys.exc_info()[0])     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Write Stream\n",
    "* Cast tweets as json data\n",
    "* Preprocess the tweets\n",
    "* Add the polarity and subjectivity to the tweets\n",
    "* Write the data from stream into memory with queryName \"tweetquery\" and save each 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- streaming is running -------\n"
     ]
    }
   ],
   "source": [
    "# Encapsulate the code in try except blocks\n",
    "try:\n",
    "    # extract the JSON stored in the topic\n",
    "    tweet_df_string = tweet_df.selectExpr(\"CAST(value AS STRING) as json_data\")\n",
    "   \n",
    "    # Preprocess the data \n",
    "    words = preprocessing(tweet_df_string)\n",
    "\n",
    "    # text classification to define polarity and subjectivity\n",
    "    words = text_classification(words)\n",
    "\n",
    "    # Repartition 'Returns a new DataFrame partitioned by the given partitioning expressions. The resulting DataFrame is hash partitioned.\n",
    "    words = words.repartition(1)    \n",
    "    \n",
    "    # Interface used to write a streaming :class:`DataFrame <pyspark.sql.DataFrame>` to external storage systems (e.g. file systems, key-value stores, etc).\n",
    "    # Write the above data into memory. Consider the entire analysis in all iteration (output mode = complete). and let the trigger runs in every 10 secs.\n",
    "    writeTweet = words.writeStream. \\\n",
    "        format(\"memory\"). \\\n",
    "        queryName(\"tweetquery\"). \\\n",
    "        outputMode(\"append\"). \\\n",
    "        trigger(processingTime='10 seconds'). \\\n",
    "        start()\n",
    "    \n",
    "    # Print banner text\n",
    "    print(\"----- streaming is running -------\")\n",
    "except:\n",
    "    # Print the error\n",
    "    print(\"Unexpected error:\", sys.exc_info()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataframe\n",
    "* Selecting all variables from tweetquery, where the location was not equal to null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encapsulate the code in try except blocks\n",
    "try:\n",
    "    df = spark.sql(\"SELECT * from tweetquery WHERE location IS NOT NULL ORDER BY created_at DESC\")\n",
    "            \n",
    "except:\n",
    "    # Print the error\n",
    "    print(\"Unexpected error:\", sys.exc_info()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting dataframe into sub-dataframes\n",
    "* Creating new dataframes for each city according to the location condition\n",
    "* Saving the dataframe as json locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_london = df.filter(lower(df['location']).contains('london'))\n",
    "df_dublin = df.filter(lower(df['location']).contains('dublin'))\n",
    "df_belfast = df.filter(lower(df['location']).contains('belfast'))\n",
    "df_manchester = df.filter(lower(df['location']).contains('manchester'))\n",
    "df_liverpool = df.filter(lower(df['location']).contains('liverpool'))\n",
    "df_miami = df.filter(lower(df['location']).contains('miami'))\n",
    "df_los_angeles = df.filter(lower(df['location']).contains('los angeles'))\n",
    "df_dallas = df.filter(lower(df['location']).contains('dallas'))\n",
    "\n",
    "dfs = [df_london, df_dublin, df_belfast, df_manchester, df_liverpool, df_miami, df_los_angeles, df_dallas]\n",
    "cities = [\"London\",\"Dublin\",\"Belfast\",\"Manchester\",\"Liverpool\",\"Miami\",\"LA\",\"Dallas\"]\n",
    "\n",
    "for i, my_df in enumerate(dfs):\n",
    "    folder_name = \"tweets_city_\" + cities[i]\n",
    "    file_path = \"./results_tweets/\" + folder_name\n",
    "    my_df.coalesce(1).write.mode('overwrite').save(file_path, format=\"json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
